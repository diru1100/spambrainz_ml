{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spam_editors_dummy_data.pickle\", \"rb\") as f:\n",
    "    spam_editors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"non_spam_editors_dummy_data.pickle\", \"rb\") as f:\n",
    "    nonspam_editors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# TODO: Figure out stopword removal\n",
    "def gather_bios(editors):\n",
    "    regex = re.compile(r\"[\\n,\\r,\\t]\")\n",
    "    out = []\n",
    "    for id, editor in editors.items():\n",
    "        if editor[\"bio\"] is not None:\n",
    "            bio = regex.sub(\"\", editor[\"bio\"])\n",
    "            out.append(bio)\n",
    "    return out\n",
    "\n",
    "\n",
    "def gather_email_domains(editors):\n",
    "    return [editor[\"email\"].split(\"@\")[1] for id, editor in editors.items()]\n",
    "\n",
    "\n",
    "def gather_website_domains(editors):\n",
    "    out = []\n",
    "    for id, editor in editors.items():\n",
    "        domain = urlparse(editor[\"website\"]).hostname\n",
    "        if domain is not None:\n",
    "            out.append(domain)\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios = gather_bios(spam_editors) + gather_bios(nonspam_editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "bio_tokenizer = Tokenizer(num_words=512)\n",
    "bio_tokenizer.fit_on_texts(bios)\n",
    "\n",
    "with open(\"bio_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(bio_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/32469562/5191080\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '&']) # remove it if you need punctuation \n",
    "# porter = PorterStemmer()\n",
    "\n",
    "\n",
    "# for doc in bios:\n",
    "#     words = []\n",
    "#     for i in wordpunct_tokenize(doc):\n",
    "#         l = i.lower()\n",
    "#         if l not in stop_words:\n",
    "#             words.append(l)\n",
    "\n",
    "# for word in words[:400]:\n",
    "#     tokens = [porter.stem(word)]\n",
    "    \n",
    "# with open(\"nltk_words.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_tokenizer = Tokenizer(num_words=1023, filters=\"\")\n",
    "email_tokenizer.fit_on_texts(gather_email_domains(spam_editors) + gather_email_domains(nonspam_editors))\n",
    "\n",
    "with open(\"email_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(email_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_tokenizer = Tokenizer(num_words=1022, filters=\"\")\n",
    "website_tokenizer.fit_on_texts(gather_website_domains(spam_editors) + gather_website_domains(nonspam_editors))\n",
    "\n",
    "with open(\"website_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(website_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "from datetime import timedelta\n",
    "from urlextract import URLExtract\n",
    "\n",
    "extractor = URLExtract()\n",
    "one_hour = timedelta(hours=1)\n",
    "\n",
    "\n",
    "def preprocess_editor(editor, spam):\n",
    "    # Apparently there are users with unset member_since\n",
    "    if editor[\"member_since\"] is not None:\n",
    "        # These shouldn't be none but you can't trust the database\n",
    "        if editor[\"last_updated\"] is not None:\n",
    "            update_delta = (editor[\"last_updated\"] - editor[\"member_since\"]) / one_hour\n",
    "        else:\n",
    "            update_delta = -1\n",
    "        \n",
    "        if editor[\"last_login_date\"] is not None:\n",
    "            login_delta = (editor[\"last_login_date\"] - editor[\"member_since\"]) / one_hour\n",
    "        else:\n",
    "            login_delta = -1\n",
    "        \n",
    "        # Confirm date may be None\n",
    "        if editor[\"email_confirm_date\"] is not None:\n",
    "            conf_delta = (editor[\"email_confirm_date\"] - editor[\"member_since\"]) / one_hour\n",
    "        else:\n",
    "            conf_delta = -1\n",
    "    else:\n",
    "        update_delta, login_delta, conf_delta = -2, -2, -2\n",
    "    \n",
    "    # Email domain\n",
    "    email_domain = email_tokenizer.texts_to_sequences([editor[\"email\"].split(\"@\")[1]])[0]\n",
    "    if len(email_domain) == 0:\n",
    "        email_token = 1024\n",
    "    else:\n",
    "        email_token = email_domain[0]\n",
    "    \n",
    "    # Website domain\n",
    "    domain = urlparse(editor[\"website\"]).hostname\n",
    "    if domain is not None:\n",
    "        website_domain = website_tokenizer.texts_to_sequences(urlparse(editor[\"website\"]).hostname)[0]\n",
    "        if len(website_domain) == 0:\n",
    "            website_token = 1023\n",
    "        else:\n",
    "            website_token = email_domain[0]\n",
    "    else:\n",
    "        website_token = 1024\n",
    "    \n",
    "    # Bio metadata\n",
    "    if editor[\"bio\"] is not None:\n",
    "        bio_len = len(editor[\"bio\"])\n",
    "        bio_urls = extractor.has_urls(editor[\"bio\"])\n",
    "        bio = bio_tokenizer.texts_to_matrix([editor[\"bio\"]], mode=\"tfidf\")[0]\n",
    "    else:\n",
    "        bio_len, bio_urls = 0, 0\n",
    "        bio = np.zeros(512)\n",
    "    \n",
    "    data = np.array([\n",
    "        spam, # spam classification\n",
    "        editor[\"area\"] is not None, # Area Set\n",
    "        editor[\"gender\"] is not None, # Gender\n",
    "        editor[\"birth_date\"] is not None, # Birth date set\n",
    "        editor[\"privs\"] != 0, # Nonzero privs\n",
    "        bio_len, # Bio length\n",
    "        bio_urls, # URLs in bio\n",
    "        conf_delta, # Confirmation delta\n",
    "        update_delta, # Last updated delta\n",
    "        login_delta, # Last login delta\n",
    "        email_token, # Email domain\n",
    "        website_token, # Website domain\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    data = np.concatenate((data, bio))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Count of editors to select from each dict\n",
    "TRAINING_COUNT = 8000\n",
    "\n",
    "training_set = np.empty((TRAINING_COUNT*2, 524))\n",
    "\n",
    "# Shuffle both dicts to prevent only picking the oldest editors\n",
    "spam_keys = list(spam_editors.keys())\n",
    "nonspam_keys = list(nonspam_editors.keys())\n",
    "\n",
    "random.shuffle(spam_keys)\n",
    "random.shuffle(nonspam_keys)\n",
    "\n",
    "# Alternate spam/nonspam\n",
    "i = 0\n",
    "j = 1\n",
    "for key in spam_keys[:TRAINING_COUNT]:\n",
    "    training_set[i] = preprocess_editor(spam_editors[key], 1)\n",
    "    i += 2\n",
    "for key in nonspam_keys[:TRAINING_COUNT]:\n",
    "    training_set[j] = preprocess_editor(nonspam_editors[key], 0)\n",
    "    j += 2\n",
    "\n",
    "with open(\"spambrainz_dataset.pickle\", \"wb\") as f:\n",
    "    pickle.dump(training_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation set\n",
    "\n",
    "EVAL_COUNT = 500\n",
    "\n",
    "eval_set = np.empty((EVAL_COUNT*2, 524))\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "for key in spam_keys[TRAINING_COUNT:EVAL_COUNT]:\n",
    "    eval_set[i] = preprocess_editor(spam_editors[key], 1)\n",
    "    i += 2\n",
    "for key in nonspam_keys[TRAINING_COUNT:EVAL_COUNT]:\n",
    "    eval_set[j] = preprocess_editor(nonspam_editors[key], 0)\n",
    "    j += 2\n",
    "\n",
    "with open(\"spambrainz_dataset_eval.pickle\", \"wb\") as f:\n",
    "    pickle.dump(eval_set, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}